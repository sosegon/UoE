% Use only LaTeX2e, calling the article.cls class and 12-point type.

\documentclass[12pt]{article}

% Users of the {thebibliography} environment or BibTeX should use the
% scicite.sty package, downloadable from *Science* at
% www.sciencemag.org/about/authors/prep/TeX_help/ .
% This package should properly format in-text
% reference calls and reference-list numbers.

\usepackage{scicite}

% Use times if you have the font installed; otherwise, comment out the
% following line.

\usepackage{times}

% The preamble here sets up a lot of new/revised commands and
% environments.  It's annoying, but please do *not* try to strip these
% out into a separate .sty file (which could lead to the loss of some
% information when we convert the file to other formats).  Instead, keep
% them in the preamble of your main LaTeX source file.


% The following parameters seem to provide a reasonable page setup.

\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm 
\textheight 21cm
\footskip 1.0cm


%The next command sets up an environment for the abstract to your paper.

\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}


% If your reference list includes text notes as well as references,
% include the following line; otherwise, comment it out.

\renewcommand\refname{References and Notes}

% The following lines set up an environment for the last note in the
% reference list, which commonly includes acknowledgments of funding,
% help, etc.  It's intended for users of BibTeX or the {thebibliography}
% environment.  Users who are hand-coding their references at the end
% using a list environment such as {enumerate} can simply add another
% item at the end, and it will be numbered automatically.

\newcounter{lastnote}
\newenvironment{scilastnote}{%
\setcounter{lastnote}{\value{enumiv}}%
\addtocounter{lastnote}{+1}%
\begin{list}%
{\arabic{lastnote}.}
{\setlength{\leftmargin}{.22in}}
{\setlength{\labelsep}{.5em}}}
{\end{list}}


% Include your paper's title here

\title{Classification applications in computer vision with human faces}


% Place the author information here.  Please hand-code the contact
% information and notecalls; do *not* use \footnote commands.  Let the
% author contact information appear immediately below the author names
% as shown.  We would also prefer that you don't change the type-size
% settings shown here.

\author
{
Sebastian Velasquez\\
s1700260\\
\normalsize{Informatics Research Review (INFR11034)}\\
}



% Include the date command, but leave its argument blank.

\date{}



%%%%%%%%%%%%%%%%% END OF PREAMBLE %%%%%%%%%%%%%%%%



\begin{document} 

% Double-space the manuscript.

\baselineskip24pt

% Make the title.

\maketitle 



% Place your abstract within the special {sciabstract} environment.

\begin{sciabstract}
The face is perhaps the part of the human body that carries the majority of visual information about a person. Several systems have exploited the data from faces to solve a variety of computer vision
tasks. In this review, I present and compare several types of information from faces in 2D images and how various applications use them to accomplish different classification tasks. Later, I provide some analysis of three systems that leverage the properties of those types of information to address different problems, and how they could be improved or used for other purposes. Finally, I provide conclusions and further work.
\end{sciabstract}


\section{Introduction}
The range of applications in computer vision that use human faces varies from entertainment to surveillance including health, sports, fashion and more. A significant number of these systems belong to the field of classification being the most relevant tasks detection, recognition, and verification. Depending on the domain, some approaches address a given task as simple as using a single feature of a subject whereas others use more complex analyses that can include correlation between different sets of attributes. 

The variability in the amount of information that every alternative use supposes that the human face is a source of large and flexible data. Depending on the features to analyze and the way systems interpret them, the results can be dramatically different. For this reason, it is essential to have a good understanding of the domain of the application to get proper results. For instance, in a system that is meant to determine the mood of a person, it makes more sense to use the facial expressions rather than the hair or eyes color.

For the common eye, it is easy to identify a set of features from almost any object (faces included). However, in computer vision, this is a non-trivial task. Several general purpose feature detectors from images have been developed e.g. \cite{harris1988combined} and \cite{lowe1999object}, as well as domain specific ones: \cite{chow1993towards} and \cite{colmenarez1999detection}. No matter their domain, the ultimate goal of these approaches is to provide information in a meaningful way for a system. This way, an application can perform a given task and provide proper results. For classification, some alternatives can extract features and categorize faces at once \cite{fleming1990categorization}.

The information extracted from faces can be used to feed a variety of systems for different purposes. Many of the alternatives use machine learning methods, being neural networks a popular choice for tasks like face detection \cite{rowley1998neural}, face recognition \cite{lawrence1997face} and face verification \cite{sun2013hybrid}; all of them intrinsically classification tasks.


\section{Face information}
Even though several approaches address the problem of feature detection and they have proved to give proper results, it is essential to understand the types of information that we can get from faces. This way it can be possible to get more insights about how some systems leverage the data from a human face and how they can be improved or modified to perform other tasks. Here, I will focus on the geometry and color information provided by faces.

\subsection{Geometry}
Geometry-wise, there are three critical characteristics of the human face: structure, vertical symmetry, and components variability. In regards to the structure, the face has a well-defined composition which means that the positions of its different components have small variance among various subjects, e.g., the mouth is close to the bottom, the eyes are close to the top, etcetera. Some applications take advantage of this property for synthesizing data to train neural networks \cite{hu2016frankenstein}, copying a makeup style from one face to another \cite{guo2009digital}, or merely replacing faces in photographies for entertainment purposes \cite{bitouk2008face}. Moreover, the well-defined structure of a face along with its enormous differences to other elements make possible systems for face detection \cite{zhu2006fast}.

The vertical symmetry of the human face is another salient geometric characteristic that several applications exploit. In the task of face recognition, this property has been used to reduce the effects of illumination in face recognition systems \cite{song2006face}. The main drawback of this property is its sensitivity to pose variations and facial expressions, an issue that dramatically reduces the performance in face recognition approaches. Several alternatives have tried to minimize these effects by using neural networks \cite{huang2000pose} and probabilistic models \cite{kanade2003multi}.

Finally, the variability of the face components from subject to subject (shape, relative position, relative size, etcetera) is high enough that it is possible to have approaches for face recognition \cite{ahonen2004face}, a task that leads to systems for authentication or re-identification. Also, the components of a face in a single subject are not static (their shapes can vary) which is evident in facial expressions. While this can be a problem for specific tasks, some applications leverage this property to recognize moods or emotions \cite{adolphs2002recognizing}. Some systems use this characteristic to facilitate human-computer interaction \cite{yacoob1996recognizing}.

Other than facial expressions, subjects can modify the geometry of their faces by using makeup, wearing accessories like glasses and earrings, or even by changing their hairstyle. This situation evidently increases the variability of a single face, which supposes a challenge for systems that rely on the geometric information of faces. Some alternatives have addressed this problem by performing the correlation between the hard face and the modified one \cite{guo2014face}. Other approaches have focused their efforts in the detection of such accessories \cite{jing2000glasses} and  \cite{wu2004glasses}.

\subsection{Color}
In computer vision, one of the most salient characteristics of color is the ability to represent it in a multitude of formats. Thanks to this property, it is possible to convert the color information in a picture from the default color space (usually represented with the red, green and blue components) to a more suitable format depending on the application domain. This attribute is leveraged by several systems for different purposes including explicit image detection \cite{basilio2011explicit} and face detection \cite{garcia1999face}. Other applications that take advantage of color information include face tracking \cite{schwerdt2000robust} and gaze tracking \cite{schiele1995gaze}.

Several types of research have widely studied the human skin color in digital images over the past years. One of the significant advances made in previous investigations in this area is the determination of the proper color spaces to study the human skin. Even though an appropriate analysis of almost any color format can lead to good results in applications to detect skin regions \cite{albiol2001optimum}, color spaces that separate the chroma and the luminance components (YCbCr) have demonstrated to provide the best results. The main reason behind these outcomes is that the luminance element (Y attribute) does not give much information about the skin and many systems discard it as stated in \cite{vezhnevets2003survey}.

Even though some skin detection applications dismiss the luminance information; other alternatives need this information to discriminate faces from other human body parts because it provides many clues about the human face. However, one of the main drawbacks in 2D images is the sensitivity of color to illumination changes which is a significant issue that reduces the performance of several systems. The main reason for such operation degradation is the use of images taken under well-controlled illumination conditions in the training stage of a model. This situation reduces the ability of a system to generalize the results with real-world images.  Some approaches try to solve this problem by using Gaussian filters and performing Principal Component Analysis (PCA) \cite{liu2005illumination} or increasing the contrast of images and filtering the best features from a face \cite{kao2010local}.

The skin is not the only source of color, other face components like the eyes and hair provide valuable information. The analysis of the data from those elements has proved to be useful in applications like gaze tracking \cite{kim1999vision} and person recognition \cite{yacoob2006detection}. One significant disadvantage related to eyes in color images is the red-eye effect caused mainly by poor light conditions and the interaction of flashlight from cameras with the choroid. Some alternatives have been proposed to correct this well-known issue \cite{volken2006automatic}.

Similar to geometric features, face color is prone to changes too. The differences can also depend on the behavior of the subjects, e.g., the use of makeup. However, most of the color variability in an image comes from the configuration of the device to capture the scene (exposure time and shutter speed), and the environment setup (lighting, shadows, etcetera). Nonetheless, skin color variability not only brings problems, but it can also be leveraged to make health-related analysis \cite{tsumura2003image}.

\section{Classification applications with faces}

There is a wide range of applications that lay their foundations in the classification task using images with faces. Based on the geometry and color information mentioned earlier, I will describe and analyze three systems that use that kind of data. The first method uses geometry information represented as a histogram of gradients (HOG) for face recognition \cite{deniz2011face}. The second system detect faces based on skin region detection using color transformations \cite{hsu2002face}. Finally, the third approach uses both geometry and color information to determine the presence of makeup in faces \cite{chen2013automatic}.

\subsection{Face recognition using HOG}

As stated in \cite{deniz2011face}, HOG is a technique founded on the number of edge orientations occurrences within the local vicinity in an image. In their work, the authors propose a method based on the previous project by \cite{albiol2008face}. The main differences with the previous work are the approaches to calculate HOG, a combination of classifiers and the databases used to ran the experiments. The different methods to perform HOG are aimed to reduce the possible dependency on the facial landmarks localization of previous results. A facial landmark is a salient feature of a face, in other words, a point of interest.

There are two proposed ways to perform HOG in a face image. The first one lies in the location of the facial landmarks as well. However, the critical point here is redundancy. Once the facial markers are localized, the system performs HOG over each one of them at different patch sizes. That means that the bigger the patch size, the higher the redundancy of the retrieved information by HOG as seen in Figure 2 in \cite{deniz2011face}. In the second method, an image face merely is divided into non overlapped patches, then HOG is calculated in each section.

As the authors mention, the redundancy of the first method may cause over-fitting if the classifier does not have a mechanism for feature selection. On the other hand, the second alternative could prevent that situation due to its non-overlapping nature. However, some amount of redundancy may be necessary to avoid the other undesired effect in machine learning models: under-fitting. In fact, some HOG implementations can be configured to create a certain amount of redundancy by adding normalization over overlapped blocks of cells.

Another option that could help to increase the robustness of the feature descriptor is the combination of several types of features. Even though the authors experimented combinations of HOG with PCA and Latent Dirichlet Allocation (LDA), other methods like Local Binary Patterns (LBP) may give good results. In fact, an alternative that combines HOG and LBP has proved to perform well in adverse situations like occlusion as seen in \cite{wang2009hog}.

The authors propose a method to make the recognition of a face based on the idea of weak classifiers, something that has proved to be right in several scenarios including the very same face recognition task \cite{yang2004face}. Another machine learning method that can perform well in this kind of problems is neural networks as demonstrated in \cite{parkhi2015deep}. However, the success of a neural network is highly tied to a significant amount of data for the training process, something that is not always possible.

Finally, the datasets used to perform the experiments present considerable differences from one to another. As seen in Figure 4 of their work, the variance of the facial landmarks is entirely different from one database to the other. This situation is not a minor issue since the performance of the method varies according to the dataset used. There is no much that can be done here since each dataset has its characteristics, some of them were even built for a specific purpose.  However, the most straightforward alternative is to combine all the databases to get a more robust classifier.

Even though the proposed method does not irrupt in the computer vision field, it was able to improve the results of its ancestor. The main point in this solution is the way to extract face features; it uses the same mechanism of the previous approach, but its execution is different. As seen in many scenarios, a proper definition of attributes can lead to expected results and verify the validity of a hypothesis, something that this work accomplished. Further work could be aimed to test the performance of the feature extraction methodology in other tasks like face detection.

\subsection{Face detection in color images}
The approach for face detection proposed by \cite{hsu2002face} is highly focused in the analysis of color in images. The solution is motivated by the lack of previous color-based alternatives that are robust enough to detect skin colors in different light conditions and complex background. The proposed method relies on two main components. The first one is aimed to detect face candidates based on some color transformations performed in an image. The second element focuses on the validation of the face candidates by detecting facial features.

The authors are aware of the challenges related to the definition of skin tones in color images due to the lighting conditions, and the previous work in the field. Based on this knowledge, the solution starts by compensating the lighting condition by using a white reference area, which allows reducing the number of false positives. As demonstrated in other related works, YCbCr is the preferred color space when it comes to human skin detection. However, unlike other solutions where the skin pixels were detected by merely checking their values against a predefined range, the algorithm performs a non-linear transformation that helps to reduce the false positives as well.

Once the face candidate region is detected, it is analyzed to find facial features. This analysis relies on the distinctive chroma and luminance components of eyes and mouth. In summary, the solution creates an intensity map that highlights the locations of the eyes and mouth. Such intensity map is formed through a sequence of nonlinear transformations of the image components in the YCbCr color space. Once the three regions are detected, all the triangle zones are computed and filtered under a threshold to identify a face. 

As described, this solution makes an intense analysis of the color in an image. However, due to the lack of semantic processing, some anthropomorphic areas can be detected as human faces, e.g., toys. Further work could consider studying the context of the image to reduce the false positives and improve results. Finally, since the approach analyzes the luminance in an image, it could be used to detect fake or mounting photos.


\subsection{Automatic Facial Makeup Detection with Application in Face Recognition}
Face recognition is a well-studied task; many geometry-based approaches have been proved to provide excellent results under certain conditions. However, in the real world, many variables can reduce the performance of such solutions. Makeup is one of those variables that challenges face recognition. The work proposed by \cite{chen2013automatic} is founded over the lack of previous solutions that address the problem of makeup and that wearing makeup is a common habit in a high percentage of the population.

In their work, they propose a method that combines a set of color, geometric and texture features aimed to detect the presence of makeup in a face. Moreover, the solution is extended to be used in face recognition. As stated in their work, wearing makeup changes the perceptual appearance of people by highlighting characteristics and reducing flaws. Several types of research have proved that this situation causes a significant impact on the performance of some recognition systems. For this reason, it is necessary to redefine the set of features used to feed those solutions.

The main contribution of this work is the redefinition of the facial features to consider the changes in shape, color, and texture due to wearing makeup. The primary objective is to provide a set of descriptors that precisely determine the presence or absence of makeup; therefore a face recognition system can be appropriately tuned to maintain its performance under such conditions.

Unlike other works that make use of the YCbCr color space when it comes to human skin, here they use the HSV color space (Hue, Saturation, Value) that highlights makeup regions in the saturation channel. The solution proposes a definition of color descriptors by combining the local and global features. This approach is a good idea since it adds some redundancy but keeps fidelity over all images. Shape and texture descriptors are simple, but they still contribute to improving the robustness of the feature vector.

Their experiments proved to give good results; however since the work was highly focused on mid-age female image samples, further actions could create a more general solution including other segments of the population. Moreover, since the approach used almost perfectly front face images, additional work can also include samples with different degrees of pose variation.

\section{Conclusions and further work}
As seen, the human face is a source of vast information. The different types of data can be leveraged to address a multitude of computer vision tasks. Despite the fact that geometric and color information have their strengths and drawbacks, rather than being mutually exclusive alternatives, they can be combined to compensate their weaknesses and maximize the performance of an application.

One of the critical aspects of systems that rely on the use of facial information is the way they represent a face. Depending on the domain of the application, some features are more useful than others. However, some solutions go beyond that; it is not just about the kind of information used to address a problem, the methods used to process and present it are very relevant to the success of an approach.

Also, it is evident that a given task can be addressed in different ways. Nonetheless, another essential component of every approach is its foundation in previous work, this way new solutions can go beyond previous proposals. In the case of applications with human faces, there is much work done to analyze their information and make use of them. However, the development does not seem to stop, and every time new alternatives are proposed. Further reviews could be aimed to investigate what kind of information from human faces could be retrieved from sources other than 2D images, including Laser Imaging Detection and Ranging (LIDAR) and medical imagery, and the applications that can be created.


% Your references go at the end of the main text, and before the
% figures.  For this document we've used BibTeX, the .bib file
% scibib.bib, and the .bst file Science.bst.  The package scicite.sty
% was included to format the reference numbers according to *Science*
% style.


\bibliography{scibib}

\bibliographystyle{Science}

\end{document}




















